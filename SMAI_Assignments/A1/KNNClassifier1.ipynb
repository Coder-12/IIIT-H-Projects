{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "import seaborn as sns\n",
    "import random as rnd\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "train_data = train_data.rename(columns={\"1\":\"labels\"})\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_label_df = pd.read_csv('test_labels.csv')\n",
    "test_label_df = test_label_df.rename(columns={\"9\":\"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def findtrainsubset(data,drop_size):\n",
    "    if isinstance(drop_size,float):\n",
    "        drop_size=round(drop_size * len(data))\n",
    "    indices = data.index.tolist()\n",
    "    drop_indices = random.sample(population=indices,k=drop_size)\n",
    "    train_subdata = data.drop(drop_indices)\n",
    "    return train_subdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_subdata = findtrainsubset(train_data,drop_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN score: 0.932933\n"
     ]
    }
   ],
   "source": [
    "train_subdataset = train_subdata.values\n",
    "X_train = train_subdataset[:,1:]\n",
    "Y_train = train_subdataset[:,0]\n",
    "X_test = test_df.values\n",
    "Y_test = test_label_df.values\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(4)\n",
    "print('KNN score: %f' % knn.fit(X_train, Y_train).score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_measure(data,predicted):\n",
    "    Precisions =[] \n",
    "    Recalls = []\n",
    "    F1_score = []\n",
    "    correct = 0\n",
    "    #print(correct)\n",
    "    confusion_matrix = [[0]*10]*10\n",
    "    for x in range(len(data)):\n",
    "        for i in range(0,10):\n",
    "            if data[x,:][0] == i:\n",
    "                for j in range(0,10):\n",
    "                    if predicted[x] == j:\n",
    "                        confusion_matrix[i][j] += 1\n",
    "    for i in range(0,10):\n",
    "        sum = TP = 0\n",
    "        for j in range(0,10):\n",
    "            if i == j:\n",
    "                TP += confusion_matrix[i][i]\n",
    "            sum += confusion_matrix[i][j]\n",
    "        recall = (TP/sum)\n",
    "        Recalls.append(recall)\n",
    "    \n",
    "    Recalls = np.array(Recalls) \n",
    "    \n",
    "    for j in range(0,10):\n",
    "        sum = TP = 0\n",
    "        for i in range(0,10):\n",
    "            if i == j:\n",
    "                TP += confusion_matrix[i][i]\n",
    "            sum += confusion_matrix[j][i]\n",
    "        precision = (TP/sum)\n",
    "        Precisions.append(precision)\n",
    "        \n",
    "    Precisions = np.array(Precisions) \n",
    "    \n",
    "    for i in range(0,10):\n",
    "        if (Precisions[i] == 0) or (Recalls[i] == 0):\n",
    "            F1_score.append(0.)\n",
    "        else:\n",
    "            F1_score.append(2/((1/Precisions[i])+(1/Recalls[i]))) \n",
    "            \n",
    "    #print(len(F1_score))\n",
    "    for x in range(len(data)):\n",
    "        if data[x,:][0] == predicted[x]:\n",
    "            correct +=1\n",
    "    Accuracy = (correct/(float)(len(data))) * 100.0\n",
    "\n",
    "    return Precisions,Recalls,F1_score,confusion_matrix,Accuracy\n",
    "\n",
    "def getresponse(neighbours):\n",
    "    classvotes = {}\n",
    "    Max = 0\n",
    "    for x in range(len(neighbours)):\n",
    "        if neighbours[x][0] in classvotes:\n",
    "            classvotes[neighbours[x][0]]+=1\n",
    "        else:\n",
    "            classvotes[neighbours[x][0]]=1\n",
    "        \n",
    "        if classvotes[neighbours[x][0]] > Max:\n",
    "            Max = classvotes[neighbours[x][0]]\n",
    "            label = neighbours[x][0]\n",
    "        \n",
    "    #sortedvotes = sorted(classvotes.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    #return sortedvotes[0][0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_algorithm(train, test):\n",
    "    output_values = [row[0] for row in train]\n",
    "    unique = list(set(output_values))\n",
    "    predicted = list()\n",
    "    for row in test:\n",
    "        index = random.randrange(len(unique))\n",
    "        predicted.append(unique[index])\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting_algorithm(train, test):\n",
    "    output_values = [row[0] for row in train]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    predicted = [prediction for i in range(len(test))]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0:\n",
      "Precision= 0.109109\n",
      "Recall= 0.109109\n",
      "F1_score= 0.109109\n",
      "\n",
      "Class 1:\n",
      "Precision= 0.102102\n",
      "Recall= 0.102102\n",
      "F1_score= 0.102102\n",
      "\n",
      "Class 2:\n",
      "Precision= 0.089089\n",
      "Recall= 0.089089\n",
      "F1_score= 0.089089\n",
      "\n",
      "Class 3:\n",
      "Precision= 0.098098\n",
      "Recall= 0.098098\n",
      "F1_score= 0.098098\n",
      "\n",
      "Class 4:\n",
      "Precision= 0.090090\n",
      "Recall= 0.090090\n",
      "F1_score= 0.090090\n",
      "\n",
      "Class 5:\n",
      "Precision= 0.112112\n",
      "Recall= 0.112112\n",
      "F1_score= 0.112112\n",
      "\n",
      "Class 6:\n",
      "Precision= 0.082082\n",
      "Recall= 0.082082\n",
      "F1_score= 0.082082\n",
      "\n",
      "Class 7:\n",
      "Precision= 0.107107\n",
      "Recall= 0.107107\n",
      "F1_score= 0.107107\n",
      "\n",
      "Class 8:\n",
      "Precision= 0.098098\n",
      "Recall= 0.098098\n",
      "F1_score= 0.098098\n",
      "\n",
      "Class 9:\n",
      "Precision= 0.112112\n",
      "Recall= 0.112112\n",
      "F1_score= 0.112112\n",
      "\n",
      "Accuracy 10.010010\n"
     ]
    }
   ],
   "source": [
    "predicted = random_algorithm(train_dataset,test_dataset)\n",
    "Precisions,Recalls,F1_score,Accuracy= get_performance_measure(test_label_dataset,predicted)\n",
    "for i in range(0,10):\n",
    "    print(\"Class %d:\" %i)\n",
    "    print(\"Precision= %f\" %Precisions[i])\n",
    "    print(\"Recall= %f\" %Recalls[i])\n",
    "    print(\"F1_score= %f\\n\" %F1_score[i])\n",
    "print(\"Accuracy %f\" %Accuracy)\n",
    "#print(\"Precision=%f\" % precision, \"Recall=%f\" %recall, \"F1-score=%f\" %F1_score,\"accuracy=%f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0:\n",
      "Precision= 0.000000\n",
      "Recall= 0.000000\n",
      "F1_score= 0.000000\n",
      "\n",
      "Class 1:\n",
      "Precision= 1.000000\n",
      "Recall= 1.000000\n",
      "F1_score= 1.000000\n",
      "\n",
      "Class 2:\n",
      "Precision= 0.000000\n",
      "Recall= 0.000000\n",
      "F1_score= 0.000000\n",
      "\n",
      "Class 3:\n",
      "Precision= 0.000000\n",
      "Recall= 0.000000\n",
      "F1_score= 0.000000\n",
      "\n",
      "Class 4:\n",
      "Precision= 0.000000\n",
      "Recall= 0.000000\n",
      "F1_score= 0.000000\n",
      "\n",
      "Class 5:\n",
      "Precision= 0.000000\n",
      "Recall= 0.000000\n",
      "F1_score= 0.000000\n",
      "\n",
      "Class 6:\n",
      "Precision= 0.000000\n",
      "Recall= 0.000000\n",
      "F1_score= 0.000000\n",
      "\n",
      "Class 7:\n",
      "Precision= 0.000000\n",
      "Recall= 0.000000\n",
      "F1_score= 0.000000\n",
      "\n",
      "Class 8:\n",
      "Precision= 0.000000\n",
      "Recall= 0.000000\n",
      "F1_score= 0.000000\n",
      "\n",
      "Class 9:\n",
      "Precision= 0.000000\n",
      "Recall= 0.000000\n",
      "F1_score= 0.000000\n",
      "\n",
      "Accuracy 10.110110\n"
     ]
    }
   ],
   "source": [
    "predicted = majority_voting_algorithm(train_dataset,test_dataset)\n",
    "Precisions,Recalls,F1_score,Accuracy= get_performance_measure(test_label_dataset,predicted)\n",
    "for i in range(0,10):\n",
    "    print(\"Class %d:\" %i)\n",
    "    print(\"Precision= %f\" %Precisions[i])\n",
    "    print(\"Recall= %f\" %Recalls[i])\n",
    "    print(\"F1_score= %f\\n\" %F1_score[i])\n",
    "print(\"Accuracy %f\" %Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validation set split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(data,valid_size):\n",
    "    if isinstance(valid_size,float):\n",
    "        valid_size=round(valid_size*len(data))\n",
    "    indices = data.index.tolist()\n",
    "    valid_indices = random.sample(population=indices,k=valid_size)\n",
    "    valid_df = data.loc[valid_indices]\n",
    "    train_df = data.drop(valid_indices)\n",
    "    return train_df,valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=400 y=3600\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "train_df,valid_df = train_valid_split(train_subdata,valid_size=0.1)\n",
    "print(\"x=%d\" %len(valid_df),\"y=%d\" %len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideandistance(train_data,Instance,length):\n",
    "    distance=0\n",
    "    for x in range(length):\n",
    "        distance+=pow((train_data[x] - Instance[x]),2)\n",
    "    return distance\n",
    "\n",
    "def manhattandistance(train_data,Instance,length):\n",
    "    distance=0\n",
    "    for x in range(length):\n",
    "        distance+= abs(train_data[x] - Instance[x])\n",
    "    return distance\n",
    "\n",
    "def getneighbours(train_data,Instance,Isparametertunning,k,proximity_measure):\n",
    "    distances = []\n",
    "    length = len(Instance)-1\n",
    "    for x in range(len(train_data)):\n",
    "        if Isparametertunning == True:\n",
    "            dist = proximity_measure(train_data[x,1:],Instance[1:],length)\n",
    "        else:\n",
    "            dist = proximity_measure(train_data[x,1:],Instance,length)\n",
    "            \n",
    "        distances.append((train_data[x,:],dist))\n",
    "        \n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    neighbours = []\n",
    "    for x in range(k):\n",
    "        neighbours.append(distances[x][0])\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>94.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>94.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>94.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   K  Accuracy\n",
       "0  3     94.50\n",
       "2  7     94.25\n",
       "1  5     94.00"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = {\"K\": [], \"Accuracy\": []}\n",
    "train_dataset = train_df.values\n",
    "valid_dataset = valid_df.values\n",
    "for k in range(3,8,2):\n",
    "    prediction = []\n",
    "    for x in range(len(valid_dataset)):\n",
    "        neighbours = getneighbours(train_dataset,valid_dataset[x,:],True,k,proximity_measure = euclideandistance)\n",
    "        result = getresponse(neighbours)\n",
    "        prediction.append(result)\n",
    "        #print('> predicted: '+ repr(result) + ', actual:' + repr(valid_data[x,:][0]))\n",
    "    _, _, _, _, accuracy = get_performance_measure(valid_dataset,prediction)\n",
    "    grid_search[\"K\"].append(k)\n",
    "    grid_search[\"Accuracy\"].append(accuracy)\n",
    "\n",
    "grid_search = pd.DataFrame(grid_search)\n",
    "grid_search.sort_values(\"Accuracy\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0:\n",
      "Precision= 0.098098\n",
      "Recall= 0.098098\n",
      "F1_score= 0.098098\n",
      "\n",
      "Class 1:\n",
      "Precision= 0.106106\n",
      "Recall= 0.106106\n",
      "F1_score= 0.106106\n",
      "\n",
      "Class 2:\n",
      "Precision= 0.107107\n",
      "Recall= 0.107107\n",
      "F1_score= 0.107107\n",
      "\n",
      "Class 3:\n",
      "Precision= 0.115115\n",
      "Recall= 0.115115\n",
      "F1_score= 0.115115\n",
      "\n",
      "Class 4:\n",
      "Precision= 0.087087\n",
      "Recall= 0.087087\n",
      "F1_score= 0.087087\n",
      "\n",
      "Class 5:\n",
      "Precision= 0.079079\n",
      "Recall= 0.079079\n",
      "F1_score= 0.079079\n",
      "\n",
      "Class 6:\n",
      "Precision= 0.101101\n",
      "Recall= 0.101101\n",
      "F1_score= 0.101101\n",
      "\n",
      "Class 7:\n",
      "Precision= 0.097097\n",
      "Recall= 0.097097\n",
      "F1_score= 0.097097\n",
      "\n",
      "Class 8:\n",
      "Precision= 0.092092\n",
      "Recall= 0.092092\n",
      "F1_score= 0.092092\n",
      "\n",
      "Class 9:\n",
      "Precision= 0.117117\n",
      "Recall= 0.117117\n",
      "F1_score= 0.117117\n",
      "\n",
      "Accuracy 93.493493\n"
     ]
    }
   ],
   "source": [
    "opt_k = 3\n",
    "prediction = []\n",
    "test_dataset = test_df.values\n",
    "test_label_dataset = test_label_df.values\n",
    "\n",
    "for x in range(len(test_dataset)):\n",
    "    neighbours = getneighbours(train_dataset,test_dataset[x,:], False, opt_k,proximity_measure = euclideandistance)\n",
    "    result = getresponse(neighbours)\n",
    "    prediction.append(result)\n",
    "    \n",
    "Precisions, Recalls, F1_score, confusion_matrix, Accuracy = get_performance_measure(test_label_dataset,prediction)\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(\"Class %d:\" %i)\n",
    "    print(\"Precision= %f\" %Precisions[i])\n",
    "    print(\"Recall= %f\" %Recalls[i])\n",
    "    print(\"F1_score= %f\\n\" %F1_score[i])\n",
    "    \n",
    "print(\"Accuracy %f\" %Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[98, 106, 107, 115, 87, 79, 101, 97, 92, 117], [98, 106, 107, 115, 87, 79, 101, 97, 92, 117], [98, 106, 107, 115, 87, 79, 101, 97, 92, 117], [98, 106, 107, 115, 87, 79, 101, 97, 92, 117], [98, 106, 107, 115, 87, 79, 101, 97, 92, 117], [98, 106, 107, 115, 87, 79, 101, 97, 92, 117], [98, 106, 107, 115, 87, 79, 101, 97, 92, 117], [98, 106, 107, 115, 87, 79, 101, 97, 92, 117], [98, 106, 107, 115, 87, 79, 101, 97, 92, 117], [98, 106, 107, 115, 87, 79, 101, 97, 92, 117]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>94.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>94.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>92.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   K  Accuracy\n",
       "0  3     94.00\n",
       "1  5     94.00\n",
       "2  7     92.75"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = {\"K\": [], \"Accuracy\": []}\n",
    "train_dataset = train_df.values\n",
    "valid_dataset = valid_df.values\n",
    "for k in range(3,8,2):\n",
    "    prediction = []\n",
    "    for x in range(len(valid_dataset)):\n",
    "        neighbours = getneighbours(train_dataset,valid_dataset[x,:],True,k,proximity_measure = manhattandistance)\n",
    "        result = getresponse(neighbours)\n",
    "        prediction.append(result)\n",
    "        \n",
    "    _, _, _, _, accuracy = get_performance_measure(valid_dataset,prediction)\n",
    "    grid_search[\"K\"].append(k)\n",
    "    grid_search[\"Accuracy\"].append(accuracy)\n",
    "\n",
    "grid_search = pd.DataFrame(grid_search)\n",
    "grid_search.sort_values(\"Accuracy\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0:\n",
      "Precision= 0.097097\n",
      "Recall= 0.097097\n",
      "F1_score= 0.097097\n",
      "\n",
      "Class 1:\n",
      "Precision= 0.116116\n",
      "Recall= 0.116116\n",
      "F1_score= 0.116116\n",
      "\n",
      "Class 2:\n",
      "Precision= 0.105105\n",
      "Recall= 0.105105\n",
      "F1_score= 0.105105\n",
      "\n",
      "Class 3:\n",
      "Precision= 0.117117\n",
      "Recall= 0.117117\n",
      "F1_score= 0.117117\n",
      "\n",
      "Class 4:\n",
      "Precision= 0.088088\n",
      "Recall= 0.088088\n",
      "F1_score= 0.088088\n",
      "\n",
      "Class 5:\n",
      "Precision= 0.078078\n",
      "Recall= 0.078078\n",
      "F1_score= 0.078078\n",
      "\n",
      "Class 6:\n",
      "Precision= 0.101101\n",
      "Recall= 0.101101\n",
      "F1_score= 0.101101\n",
      "\n",
      "Class 7:\n",
      "Precision= 0.098098\n",
      "Recall= 0.098098\n",
      "F1_score= 0.098098\n",
      "\n",
      "Class 8:\n",
      "Precision= 0.086086\n",
      "Recall= 0.086086\n",
      "F1_score= 0.086086\n",
      "\n",
      "Class 9:\n",
      "Precision= 0.113113\n",
      "Recall= 0.113113\n",
      "F1_score= 0.113113\n",
      "\n",
      "Accuracy 92.192192\n"
     ]
    }
   ],
   "source": [
    "opt_k = 3\n",
    "prediction = []\n",
    "test_dataset = test_df.values\n",
    "test_label_dataset = test_label_df.values\n",
    "\n",
    "for x in range(len(test_dataset)):\n",
    "    neighbours = getneighbours(train_dataset,test_dataset[x,:], False, opt_k,proximity_measure = manhattandistance)\n",
    "    result = getresponse(neighbours)\n",
    "    prediction.append(result)\n",
    "    \n",
    "Precisions, Recalls, F1_score, confusion_matrix, Accuracy = get_performance_measure(test_label_dataset,prediction)\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(\"Class %d:\" %i)\n",
    "    print(\"Precision= %f\" %Precisions[i])\n",
    "    print(\"Recall= %f\" %Recalls[i])\n",
    "    print(\"F1_score= %f\\n\" %F1_score[i])\n",
    "    \n",
    "print(\"Accuracy %f\" %Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97, 116, 105, 117, 88, 78, 101, 98, 86, 113], [97, 116, 105, 117, 88, 78, 101, 98, 86, 113], [97, 116, 105, 117, 88, 78, 101, 98, 86, 113], [97, 116, 105, 117, 88, 78, 101, 98, 86, 113], [97, 116, 105, 117, 88, 78, 101, 98, 86, 113], [97, 116, 105, 117, 88, 78, 101, 98, 86, 113], [97, 116, 105, 117, 88, 78, 101, 98, 86, 113], [97, 116, 105, 117, 88, 78, 101, 98, 86, 113], [97, 116, 105, 117, 88, 78, 101, 98, 86, 113], [97, 116, 105, 117, 88, 78, 101, 98, 86, 113]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
