{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "import seaborn as sns\n",
    "import random as rnd\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "from pprint import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nres = []\\ntest_data = test_df.values\\nfor x in range(len(test_df.columns)):\\n    cnt=0\\n    for i in test_data[:,x]:\\n        if i == '?':\\n            cnt+=1\\n    res.append(cnt)\\n#print(test_data[6,:])\\nprint(res[10])\\n#print(len(train_data))\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"C:\\\\Users\\\\Aklesh Mishra\\\\q2\\\\train.csv\",header=None)\n",
    "train_data = train_data.rename(columns={0:\"class\"})\n",
    "\n",
    "# Replace missing values by mode of the column with the missing values\n",
    "miscol_mode = train_data.mode()[11]\n",
    "train_data = train_data.fillna({11:miscol_mode})\n",
    "\n",
    "'''\n",
    "# Dr\n",
    "drop_index = []\n",
    "for i in range(len(train_data)):\n",
    "    if train_data.iloc[i,11] == '?':\n",
    "        drop_index.append(i)\n",
    "df = train_data\n",
    "for i in drop_index:\n",
    "    df = df.drop(train_data.index[i])\n",
    "train_data = df\n",
    "'''\n",
    "test_df = pd.read_csv(\"C:\\\\Users\\\\Aklesh Mishra\\\\q2\\\\test.csv\",header=None)\n",
    "miscol_mode = test_df.mode()[10]\n",
    "test_df = test_df.fillna({10:miscol_mode})\n",
    "test_label_df = pd.read_csv(\"C:\\\\Users\\\\Aklesh Mishra\\\\q2\\\\test_labels.csv\",header=None)\n",
    "test_label_df = test_label_df.rename(columns={0:\"class\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN score: 38.788800\n"
     ]
    }
   ],
   "source": [
    "train = train_data.values\n",
    "X_train = train[:,1:]\n",
    "Y_train = train[:,0]\n",
    "X_test = test_df.values\n",
    "Y_test = test_label_df.values\n",
    "k=4\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(X_train,Y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "scores = metrics.accracy_score(Y_test,Y_pred)\n",
    "print('KNN score: %f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(train_data,valid_size):\n",
    "    if isinstance(valid_size,float):\n",
    "        valid_size=round(valid_size * len(train_data))\n",
    "    indices = train_data.index.tolist()\n",
    "    valid_indices = random.sample(population=indices,k=valid_size)\n",
    "    valid_df = train_data.loc[valid_indices]\n",
    "    train_df = train_data.drop(valid_indices)\n",
    "    return train_df,valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_df,valid_df = train_valid_split(train_data,valid_size=0.3)\n",
    "train_dataset = train_df.values\n",
    "valid_dataset = valid_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_measure(data,predicted):\n",
    "    correct = 0\n",
    "    TP = TN = FP = FN = 0\n",
    "    \n",
    "    for x in range(len(data)):\n",
    "        if data[x,:][0] == 'e':\n",
    "            if predicted[x] == 'e':\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            if predicted[x] == 'e':\n",
    "                FP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "    \n",
    "    precision = (TP/(float)(TP + FP))\n",
    "    recall = (TP/(float)(TP + FN))\n",
    "    F1_score = 2/(float)((1/precision)+(1/recall))   \n",
    "    \n",
    "    for x in range(len(data)):\n",
    "        if data[x,:][0] == predicted[x]:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = (correct/(float)(len(data))) * 100.0\n",
    "    \n",
    "    return precision, recall, F1_score, accuracy\n",
    "\n",
    "def getresponse(neighbours):\n",
    "    classvotes = {}\n",
    "    Max = 0\n",
    "    for x in range(len(neighbours)):\n",
    "        if neighbours[x][0] in classvotes:\n",
    "            classvotes[neighbours[x][0]]+=1\n",
    "        else:\n",
    "            classvotes[neighbours[x][0]]=1\n",
    "        '''\n",
    "        if classvotes[neighbours[x][0]] > Max:\n",
    "            Max = classvotes[neighbours[x][0]]\n",
    "            label = neighbours[x][0]\n",
    "        '''\n",
    "    sortedvotes = sorted(classvotes.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedvotes[0][0]\n",
    "    #return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_algorithm(train, test):\n",
    "    output_values = [row[0] for row in train]\n",
    "    unique = list(set(output_values))\n",
    "    predicted = list()\n",
    "    for row in test:\n",
    "        index = random.randrange(len(unique))\n",
    "        predicted.append(unique[index])\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting_algorithm(train, test):\n",
    "    output_values = [row[-1] for row in train]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    predicted = [prediction for i in range(len(test))]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping the missing values of the columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision=0.254279 Recall=0.205941 F1-score=0.227571 accuracy=29.400000\n"
     ]
    }
   ],
   "source": [
    "predicted = random_algorithm(train_dataset,test_dataset)\n",
    "precision, recall, F1_score, accuracy = get_performance_measure(test_label_data,prediction)\n",
    "print(\"Precision=%f\" % precision, \"Recall=%f\" %recall, \"F1-score=%f\" %F1_score,\"accuracy=%f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision=0.254279 Recall=0.205941 F1-score=0.227571 accuracy=29.400000\n"
     ]
    }
   ],
   "source": [
    "predicted = majority_voting_algorithm(train_dataset,test_dataset)\n",
    "precision, recall, F1_score, accuracy = get_performance_measure(test_label_data,prediction)\n",
    "print(\"Precision=%f\" % precision, \"Recall=%f\" %recall, \"F1-score=%f\" %F1_score,\"accuracy=%f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing missing values with mode of the column with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision=0.286017 Recall=0.267327 F1-score=0.276356 accuracy=29.300000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_df.values\n",
    "valid_dataset = valid_df.values\n",
    "predicted = random_algorithm(train_dataset,test_dataset)\n",
    "precision, recall, F1_score, accuracy = get_performance_measure(test_label_data,prediction)\n",
    "print(\"Precision=%f\" % precision, \"Recall=%f\" %recall, \"F1-score=%f\" %F1_score,\"accuracy=%f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision=0.286017 Recall=0.267327 F1-score=0.276356 accuracy=29.300000\n"
     ]
    }
   ],
   "source": [
    "predicted = majority_voting_algorithm(train_dataset,test_dataset)\n",
    "precision, recall, F1_score, accuracy = get_performance_measure(test_label_data,prediction)\n",
    "print(\"Precision=%f\" % precision, \"Recall=%f\" %recall, \"F1-score=%f\" %F1_score,\"accuracy=%f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_measure(train_data,Instance,length):\n",
    "    matches=0\n",
    "    for x in range(length):\n",
    "        if train_data[x] == Instance[x]:\n",
    "            matches += 1\n",
    "    distance = (matches/(float)(len(train_data)));\n",
    "    return distance;\n",
    "\n",
    "def getneighbours(train_data,Instance,Isparametertunning,k):\n",
    "    distances = []\n",
    "    length = len(Instance)-1\n",
    "    for x in range(len(train_data)):\n",
    "        if Isparametertunning == True:\n",
    "            dist = proximity_measure(train_data[x,1:],Instance[1:],length)\n",
    "        else:\n",
    "            dist = proximity_measure(train_data[x,1:],Instance,length)\n",
    "\n",
    "        distances.append((train_data[x,:],dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    neighbours = []\n",
    "    for x in range(k):\n",
    "        neighbours.append(distances[x][0])\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>34.707904</td>\n",
       "      <td>0.257703</td>\n",
       "      <td>0.231738</td>\n",
       "      <td>0.244032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>32.989691</td>\n",
       "      <td>0.226744</td>\n",
       "      <td>0.196474</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>32.760596</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.191436</td>\n",
       "      <td>0.205683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>32.646048</td>\n",
       "      <td>0.221574</td>\n",
       "      <td>0.191436</td>\n",
       "      <td>0.205405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>32.416953</td>\n",
       "      <td>0.223496</td>\n",
       "      <td>0.196474</td>\n",
       "      <td>0.209115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     K   Accuracy  Precision    Recall  F1-score\n",
       "17  19  34.707904   0.257703  0.231738  0.244032\n",
       "15  17  32.989691   0.226744  0.196474  0.210526\n",
       "14  16  32.760596   0.222222  0.191436  0.205683\n",
       "16  18  32.646048   0.221574  0.191436  0.205405\n",
       "13  15  32.416953   0.223496  0.196474  0.209115"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = {\"K\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-score\": []}\n",
    "train_dataset = train_df.values\n",
    "valid_dataset = valid_df.values\n",
    "for k in range(2,20):\n",
    "    prediction = []\n",
    "    for x in range(len(valid_dataset)):\n",
    "        neighbours = getneighbours(train_dataset,valid_dataset[x,:],True,k)\n",
    "        result = getresponse(neighbours)\n",
    "        prediction.append(result)\n",
    "        #print('> predicted: '+ repr(result) + ', actual:' + repr(valid_dataset[x,:][0]))\n",
    "    precision, recall, F1_score, accuracy = get_performance_measure(valid_dataset,prediction)\n",
    "    grid_search[\"K\"].append(k)\n",
    "    grid_search[\"Accuracy\"].append(accuracy)\n",
    "    grid_search[\"Precision\"].append(precision)\n",
    "    grid_search[\"Recall\"].append(recall)\n",
    "    grid_search[\"F1-score\"].append(F1_score)\n",
    "    \n",
    "grid_search = pd.DataFrame(grid_search)\n",
    "grid_search.sort_values(\"Accuracy\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing missing values by mode of the column with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision=0.151515 Recall=0.118812 F1-score=0.133185 accuracy=21.900000\n"
     ]
    }
   ],
   "source": [
    "opt_k = 19\n",
    "prediction = []\n",
    "test_dataset = test_df.values\n",
    "test_label_data = test_label_df.values\n",
    "for x in range(len(test_dataset)):\n",
    "    neighbours = getneighbours(train_dataset,test_dataset[x,:], False, opt_k)\n",
    "    result = getresponse(neighbours)\n",
    "    prediction.append(result)\n",
    "precision, recall, F1_score, accuracy = get_performance_measure(test_label_data,prediction)\n",
    "print(\"Precision=%f\" % precision, \"Recall=%f\" %recall, \"F1-score=%f\" %F1_score,\"accuracy=%f\" %accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision=0.286017 Recall=0.267327 F1-score=0.276356 accuracy=29.300000\n"
     ]
    }
   ],
   "source": [
    "opt_k = 19\n",
    "prediction = []\n",
    "test_dataset = test_df.values\n",
    "test_label_data = test_label_df.values\n",
    "for x in range(len(test_dataset)):\n",
    "    neighbours = getneighbours(train_dataset,test_dataset[x,:], False, opt_k)\n",
    "    result = getresponse(neighbours)\n",
    "    prediction.append(result)\n",
    "precision, recall, F1_score, accuracy = get_performance_measure(test_label_data,prediction)\n",
    "print(\"Precision=%f\" % precision, \"Recall=%f\" %recall, \"F1-score=%f\" %F1_score,\"accuracy=%f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_plot(train_df, title=\"Training Data\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
